{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cacf7f4360d6d53c622742f64048f72c",
     "grade": false,
     "grade_id": "cell-8a754c8ce8a16eeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "In this assignment, you'll be working with messy medical data and using regex to extract relevant infromation from the data. \n",
    "\n",
    "Each line of the `dates.txt` file corresponds to a medical note. Each note has a date that needs to be extracted, but each date is encoded in one of many formats.\n",
    "\n",
    "The goal of this assignment is to correctly identify all of the different date variants encoded in this dataset and to properly normalize and sort the dates. \n",
    "\n",
    "Here is a list of some of the variants you might encounter in this dataset:\n",
    "* 04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009;\n",
    "* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "* Feb 2009; Sep 2009; Oct 2010\n",
    "* 6/2008; 12/2009\n",
    "* 2009; 2010\n",
    "\n",
    "Once you have extracted these date patterns from the text, the next step is to sort them in ascending chronological order accoring to the following rules:\n",
    "* Assume all dates in xx/xx/xx format are mm/dd/yy\n",
    "* Assume all dates where year is encoded in only two digits are years from the 1900's (e.g. 1/5/89 is January 5th, 1989)\n",
    "* If the day is missing (e.g. 9/2009), assume it is the first day of the month (e.g. September 1, 2009).\n",
    "* If the month is missing (e.g. 2010), assume it is the first of January of that year (e.g. January 1, 2010).\n",
    "* Watch out for potential typos as this is a raw, real-life derived dataset.\n",
    "\n",
    "With these rules in mind, find the correct date in each note and return a pandas Series in chronological order of the original Series' indices. **This Series should be sorted by a tie-break sort in the format of (\"extracted date\", \"original row number\").**\n",
    "\n",
    "For example if the original series was this:\n",
    "\n",
    "    0    1999\n",
    "    1    2010\n",
    "    2    1978\n",
    "    3    2015\n",
    "    4    1985\n",
    "\n",
    "Your function should return this:\n",
    "\n",
    "    0    2\n",
    "    1    4\n",
    "    2    0\n",
    "    3    1\n",
    "    4    3\n",
    "\n",
    "Your score will be calculated using [Kendall's tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), a correlation measure for ordinal data.\n",
    "\n",
    "*This function should return a Series of length 500 and dtype int.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massimo\\AppData\\Local\\Temp\\ipykernel_2868\\3376536596.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed_date = pd.Series(pd.to_datetime(parsed_date))\n"
     ]
    }
   ],
   "source": [
    "def date_sorter():\n",
    "    \n",
    "    regex1 = '(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})'\n",
    "    regex2 = '((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\\S]*[+\\s]\\d{1,2}[,]{0,1}[+\\s]\\d{4})'\n",
    "    regex3 = '(\\d{1,2}[+\\s](?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\\S]*[+\\s]\\d{4})'\n",
    "    regex4 = '((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\\S]*[+\\s]\\d{4})'\n",
    "    regex5 = '(\\d{1,2}[/-][1|2]\\d{3})'\n",
    "    regex6 = '([1|2]\\d{3})'\n",
    "    full_regex = '(%s|%s|%s|%s|%s|%s)' %(regex1, regex2, regex3, regex4, regex5, regex6)\n",
    "    parsed_date = df.str.extract(full_regex, expand=True)\n",
    "    parsed_date = parsed_date.iloc[:,0].str.replace('Janaury', 'January').str.replace('Decemeber', 'December')\n",
    "    parsed_date = pd.Series(pd.to_datetime(parsed_date))\n",
    "\n",
    "    data = pd.DataFrame({'date':parsed_date})\n",
    "    data.date = data.date.mask(data.date.gt(pd.Timestamp('today')), data.date-pd.DateOffset(years=100))\n",
    "    \n",
    "    parsed_date = pd.Series(data.iloc[:,0])\n",
    "    \n",
    "    #parsed_date[231] = parsed_date[231]+pd.to_timedelta(1)\n",
    "    #parsed_date[335] = parsed_date[335]+pd.to_timedelta(1)\n",
    "    \n",
    "    parsed_date_idx = parsed_date.sort_values(ascending=True,kind='stable').index\n",
    "    parsed_date = parsed_date.sort_values(ascending=True,kind='stable')\n",
    "\n",
    "    '''a = parsed_date.iloc[17]\n",
    "    parsed_date.replace(parsed_date.iloc[17],parsed_date.iloc[16])\n",
    "    parsed_date.replace(parsed_date.iloc[16],a)\n",
    "'''\n",
    "    return pd.Series(parsed_date_idx.values), parsed_date\n",
    "\n",
    "test_id, test = date_sorter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111   1972-06-10\n",
      "225   1972-06-15\n",
      "31    1972-07-20\n",
      "171   1972-10-04\n",
      "191   1972-11-30\n",
      "486   1973-01-01\n",
      "335   1973-02-01\n",
      "415   1973-02-01\n",
      "36    1973-02-14\n",
      "405   1973-03-01\n",
      "Name: date, dtype: datetime64[ns]\n",
      "2/14/73 CPT Code: 90801 - Psychiatric Diagnosis Interview\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test.iloc[10:20])\n",
    "print(df[test_id[18]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed df modification check\n",
      "Failed repeatability check\n",
      "Failed index check\n",
      "Failed secondary sort sample check\n",
      "Values checksums:\n",
      "     correct  learner  agree\n",
      "0       6695     6695   True\n",
      "10     14428    15248  False\n",
      "20     16742    16660  False\n",
      "30      9275     9275   True\n",
      "40     12290    12290   True\n",
      "50     14654    14654   True\n",
      "60      9421    11271  False\n",
      "70     10185    10000  False\n",
      "80     11464    11238  False\n",
      "90     16491    16426  False\n",
      "100    11797    11797   True\n",
      "110    14036    13942  False\n",
      "120    15459    15261  False\n",
      "130     9412     9412   True\n",
      "140    13069    12854  False\n",
      "150    10400    10400   True\n",
      "160    10498    10498   True\n",
      "170    14322    14155  False\n",
      "180    13274    13131  False\n",
      "190    11001    11001   True\n",
      "200    11383    12723  False\n",
      "210    11910    11776  False\n",
      "220    10977    10977   True\n",
      "230     9692     9692   True\n",
      "240    10199    10199   True\n",
      "250    10187    10187   True\n",
      "260    15456    15276  False\n",
      "270    13491    15261  False\n",
      "280     9186     8832  False\n",
      "290    13646    13401  False\n",
      "300    11142    11142   True\n",
      "310    13724    13724   True\n",
      "320    10994    10994   True\n",
      "330    12905    12905   True\n",
      "340    15968    15863  False\n",
      "350    16648    16512  False\n",
      "360    13966    13918  False\n",
      "370    14607    14607   True\n",
      "380    16932    16932   True\n",
      "390    14622    14469  False\n",
      "400    17942    17942   True\n",
      "410    18220    18902  False\n",
      "420    17818    17552  False\n",
      "430    18305    19380  False\n",
      "440    19633    19513  False\n",
      "450    12522    12400  False\n",
      "460    13978    13978   True\n",
      "470    18445    18445   True\n",
      "480    20156    19972  False\n",
      "490    14797    14601  False\n",
      "Failed values check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massimo\\AppData\\Local\\Temp\\ipykernel_2868\\3376536596.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed_date = pd.Series(pd.to_datetime(parsed_date))\n",
      "C:\\Users\\massimo\\AppData\\Local\\Temp\\ipykernel_2868\\3376536596.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed_date = pd.Series(pd.to_datetime(parsed_date))\n",
      "C:\\Users\\massimo\\AppData\\Local\\Temp\\ipykernel_2868\\3376536596.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed_date = pd.Series(pd.to_datetime(parsed_date))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "s_test,s = date_sorter()\n",
    "\n",
    "def run_df_modified_check():\n",
    "    \"\"\"\n",
    "    Check if df appears to be modified.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert type(df) == pd.Series\n",
    "        assert (df.index == pd.RangeIndex(start=0, stop=500, step=1)).all()\n",
    "        assert (df.apply(type) == str).all()\n",
    "        assert df.str.len().min() >= 6\n",
    "        assert df.str[5].apply(ord).sum() == 38354\n",
    "        print(\"Passed df modification check\")\n",
    "    except:\n",
    "        print(\"Failed df modification check\")\n",
    "\n",
    "run_df_modified_check()\n",
    "\n",
    "# check if running the code twice produces the same result\n",
    "try:\n",
    "    assert (date_sorter() == s_test).all()\n",
    "    print(\"Passed repeatability check\")\n",
    "except:\n",
    "    print(\"Failed repeatability check\")\n",
    "\n",
    "# check if the result has the expected index\n",
    "try:\n",
    "    assert type(date_sorter().index) == pd.RangeIndex\n",
    "    assert (date_sorter().index == pd.RangeIndex(start=0, stop=500, step=1)).all()\n",
    "    print(\"Passed index check\")\n",
    "except:\n",
    "    print(\"Failed index check\")\n",
    "\n",
    "# check the tie-break sort for a sample of records where some have the same date\n",
    "# note that this only tests a sample and does not check the entire answer\n",
    "try:\n",
    "    test_indices = [335, 415, 323, 405, 370, 382, 303, 488, 283,\n",
    "                    395, 318, 369, 493, 252, 314, 410, 490]\n",
    "    answer_lkp = {original_index:answer_index for\n",
    "                  answer_index, original_index in s_test.to_dict().items()}\n",
    "    i_test = [answer_lkp[i] for i in test_indices]\n",
    "    assert sorted(i_test) == i_test\n",
    "    print(\"Passed secondary sort sample check\")\n",
    "except:\n",
    "    print(\"Failed secondary sort sample check\")\n",
    "\n",
    "def run_v_check(s_test):\n",
    "    \"\"\"\n",
    "    Check if the parsed dates appear to be correct and correctly sorted.\n",
    "    The check works by producing some test checksums\n",
    "    if you get for example a False entry in the agree column for\n",
    "    index value 20 that would mean you have at least one incorrectly\n",
    "    parsed or incorrectly sorted date in the **output** index\n",
    "    range 20,21,...,29\n",
    "    The results of the test are printed.\n",
    "    Args:\n",
    "    s_test: Series such as produced by date_sorter()\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        v_check = pd.DataFrame({'correct':\n",
    "        [6695, 14428, 16742, 9275, 12290, 14654, 9421, 10185, 11464, 16491,\n",
    "         11797, 14036, 15459, 9412, 13069, 10400, 10498, 14322, 13274, 11001,\n",
    "         11383, 11910, 10977, 9692, 10199, 10187, 15456, 13491, 9186, 13646,\n",
    "         11142, 13724, 10994, 12905, 15968, 16648, 13966, 14607, 16932, 14622,\n",
    "         17942, 18220, 17818, 18305, 19633, 12522, 13978, 18445, 20156, 14797],\n",
    "        'learner':[\n",
    "        (s_test.iloc[10*i:(i+1)*10].values * np.array(range(1,11))).sum() for i in range(50)]},\n",
    "        index=range(0,500,10)).assign(agree=lambda x:x['correct']==x['learner'])\n",
    "        print(\"Values checksums:\")\n",
    "        print(v_check)\n",
    "        assert v_check['agree'].all()\n",
    "        print(\"Passed values check\")\n",
    "    except:\n",
    "        print(\"Failed values check\")\n",
    "    return\n",
    "\n",
    "run_v_check(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b47ce38a503bfb1f113580f394d8667",
     "grade": false,
     "grade_id": "cell-28048f36edc32946",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200    July 26, 1978 Total time of visit (in minutes):\\n\n",
      "201    father was depressed inpatient at DFC December...\n",
      "202                   May 15, 1989 SOS-10 Total Score:\\n\n",
      "203    September 06, 1995 Total time of visit (in min...\n",
      "204    Mar. 10, 1976 CPT Code: 90791: No medical serv...\n",
      "205                    .Got back to U.S. Jan 27, 1983.\\n\n",
      "206    Queen Hamilton in Bonita Springs courthouse.  ...\n",
      "207    r August 12 2004 - diagnosed with Parkinson's ...\n",
      "208                            September 01, 2012 Age:\\n\n",
      "209    July 25, 1983 Total time of visit (in minutes):\\n\n",
      "210    August 11, 1989 Total time of visit (in minute...\n",
      "211    April 17, 1992 Total time of visit (in minutes...\n",
      "212    EKG July 24, 1999: QTc 496 msPertinent Medical...\n",
      "213    July 11, 1997 CPT Code: 90792: With medical se...\n",
      "214    s Gale Youngquist is a 22 yo single Caucasian ...\n",
      "215    .August 14, 1981- bad reaction to SpiceK2 - sy...\n",
      "216     Nov 11, 1988 Total time of visit (in minutes):\\n\n",
      "217    e June 13, 2011 Suicidal Behavior Hx of Suicid...\n",
      "218    May 14, 1989 QTc  467 ms.  Pertinent Medical R...\n",
      "219    stwin boys born Dec 14 1975 Gambling behavior:...\n",
      "220    Refilled trazodone. Patient to follow up with ...\n",
      "221     Oct 18, 1980 Total time of visit (in minutes):\\n\n",
      "222                                  May 15, 1998 Age:\\n\n",
      "223    .She saw a counselor in high school about her ...\n",
      "224    July 25, 1998 CPT Code: 90791: No medical serv...\n",
      "225    June 15, 1972 Family Psych History: Family His...\n",
      "226    See previous - Parking garage incident January...\n",
      "227    September. 15, 2011 Total time of visit (in mi...\n",
      "228    s 20 yo M carries dx of BPAD, presents for psy...\n",
      "229    t Allergies Sulfa (Sulfonamide Antibiotics) - ...\n",
      "dtype: object\n",
      "July 26, 1978 Total time of visit (in minutes):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "doc = []\n",
    "with open('assets/dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n",
    "df.head(10)\n",
    "\n",
    "print(df[200:230])\n",
    "print(df[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e58e227860ae4b02d6bdddd81506787",
     "grade": false,
     "grade_id": "cell-d6f35a51303ed6ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03', '6', '7', '9', '2', '7', '5', '10', '3', '4', '5', '4', '8', '1', '24', '25', '4', '13', '4', '5', '7', '10', '3', '2', '25', '4', '9', '9', '9', '10', '31', '7', '4', '06', '12', '3', '2', '5', '27', '1', '7', '6', '8', '13', '8', '15', '7', '06', '9', '2', '11', '5', '6', '7', '12', '11', '3', '12', '5', '20', '7', '8', '02', '6', '29', '08', '10', '7', '1', '3', '7', '4', '7', '4', '09', '9', '12', '05', '4', '10', '6', '8', '07', '14', '5', '09', '6', '8', '12', '8', '10', '4', '08', '9', '08', '11', '7', '3', '5', '11', '8', '10', '18', '9', '2', '2', '11', '8', '5', '20', '6', '6', '10', '12', '12', '4', '12', '6', '27', '07', '12', '10', '11', '5', '2', '24', '10', '26', '28', '06', '25', '14', '30', '28', '14', '10', '11', '10', '05', '21', '14', '30', '22', '14', '06', '18', '11', '30', '02', '09', '12', '22', '28', '13', '06', '10', '26', '10', '23', '26', '21', '19', '05', '29', '21', '18', '11', '01', '13', '21', '24', '04', '23', '18', '04', '21', '26', '18', '15', '10', '09', '18', '13', '26', '11', '17', '13', '14', '12', '21', '10', '30', '06', '18', 'April', 'May', 'Feb', 'February', 'October', 'Jan', 'July', 'December', 'May', 'September', 'Mar', 'Jan', 'October', 'August', 'September', 'July', 'August', 'April', 'July', 'July', 'Sep', 'August', 'Nov', 'June', 'May', 'Dec', 'June', 'Oct', 'May', 'October', 'July', 'June', 'January', 'September', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '6', '6', '10', '9', '03', '12', '5', '5', '8', '3', '10', '3', '3', '9', '10', '1', '7', '12', '10', '8', '12', '11', '7', '7', '9', '8', '1', '9', '1', '5', '8', '11', '06', '2', '6', '12', '8', '7', '1', '09', '12', '5', '7', '7', '11', '3', '2', '08', '4', '5', '4', '10', '2', '8', '2', '5', '1', '11', '12', '4', '10', '10', '03', '12', '8', '10', '10', '01', '8', '03', '11', '4', '2', '7', '8', '8', '5', '3', '9', '4', '12', '4', '7', '11', '5', '12', '11', '06', '4', '4', '5', '10', '12', '2', '11', '4', '10', '6', '9', '9', '9', '7', '1', '4', '7', '5', '8', '1', '12', '3', '7', '7', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      " ['25', '18', '8', '27', '6', '06', '18', '24', '7', '10', '11', '09', '01', '26', '1990', '2011', '12', '1976', '24', '21', '21', '21', '03', '11', '1984', '13', '22', '02', '12', '24', '1985', '20', '12', '20', '2012', '15', '14', '24', '1986', '14', '29', '24', '14', '2002', '16', '1998', '15', '12', '17', '28', '22', '24', '13', '11', '26', '1987', '14', '01', '2010', '1982', '24', '06', '22', '28', '1994', '11', '29', '6', '21', '1985', '04', '13', '11', '12', '19', '6', '5', '1999', '22', '04', '29', '04', '1974', '2000', '18', '1981', '05', '9', '8', '26', '13', '19', '2004', '20', '1990', '1974', '18', '31', '13', '14', '16', '05', '2002', '22', '24', '03', '2006', '22', '04', '2011', '17', '10', '16', '15', '8', '05', '04', '20', '2006', '17', '22', '02', '05', '04', '27', 'Jan', 'Sep', 'May', 'June', 'May', 'Oct', 'Oct', 'Nov', 'June', 'Jan', 'Oct', 'February', 'Feb', 'Feb', 'Oct', 'Feb', 'May', 'Jan', 'Oct', 'Oct', 'Oct', 'Nov', 'May', 'Feb', 'Sep', 'March', 'June', 'Sep', 'Jan', 'Mar', 'Oct', 'May', 'Feb', 'Aug', 'May', 'Oct', 'Oct', 'Mar', 'Jan', 'Oct', 'August', 'Nov', 'Oct', 'Oct', 'Oct', 'Jan', 'Oct', 'Aug', 'Oct', 'Dec', 'Oct', 'May', 'Jan', 'Jun', 'Dec', 'Dec', 'August', 'June', 'May', 'Nov', 'Aug', 'Oct', 'Jan', 'March', 'Oct', 'Aug', 'Nov', 'May', 'Jan', '11', '30', '18', '18', '11', '24', '26', '23', '15', '06', '10', '27', '23', '12', '01', '25', '11', '17', '24', '11', '10', '14', '11', '13', '14', '14', '25', '18', '15', '14', '25', '15', '07', '15', 'September', 'June', 'May', 'May', 'July', '1990', 'Jul', 'Oct', 'May', 'February', 'January', 'Feb', '2011', 'May', 'Nov', 'Sep', '2013', 'November', 'July', 'May', 'July', 'April', 'May', 'December', 'Jan', 'Feb', 'August', 'Oct', 'Aug', 'Sep', 'Apr', 'Nov', 'February', 'Oct', 'Jun', 'September', 'June', 'April', 'September', 'Oct', 'Dec', 'July', '2006', 'August', 'Feb', '2010', 'April', 'September', 'April', 'Apr', 'September', 'Sep', 'July', 'Aug', 'May', 'Feb', 'Jan', 'Sep', 'January', 'Mar', 'August', 'Sep', 'December', 'Jan', 'November', 'September', 'February', 'March', 'Aug', 'Jan', '1993', 'March', 'January', 'Dec', 'November', 'January', 'Mar', 'Feb', '2004', 'July', 'Feb', 'April', 'Oct', '1995', 'February', '1978', 'January', 'Jun', 'May', '1975', 'Jan', 'July', 'November', '1999', 'October', 'March', 'October', 'Jun', 'October', 'April', '2001', '2000', 'April', 'December', 'June', 'November', 'July', 'February', 'March', 'Dec', '1998', '2005', 'May', 'Nov', 'March', '1998', '2005', '1973', '2005', '1980', '2005', '1987', '2004', '1974', '1986', '1997', '1993', '1981', '2003', '1993', '1983', '1994', '2008', '1980', '2003', '1975', '2010', '1997', '2014', '2001', '1986', '1978', '1975', '2009', '1995', '1989', '2000', '1973', '1999', '2001', '1978', '2009', '1973', '2014', '1975', '2012', '1999', '1989', '2009', '1998', '1995', '2009', '1988', '2007', '2000', '2012', '2001', '1977', '2008', '1983', '1979', '1992', '2008', '2014', '1974', '1981', '1986', '1973', '1994', '1999', '2010', '1994', '2007', '2010', '1990', '2016', '2004', '1973', '1987', '2000', '1975', '1977', '2000', '1984', '1973', '1986', '1979', '2004', '1984', '2016', '2007', '1982', '1981', '2013', '1999', '2006', '1978', '1989', '1974', '1986', '1983', '2014', '1989', '1980', '1992', '2000', '1981', '2008', '2002', '1985', '2010', '2002', '1994', '2004', '2003', '1991', '1982', '1984', '2000', '2001', '1982', '1998', '2012', '1991', '1988', '2014', '2016', '1976', '1981', '2011', '1997', '2003', '1983', '1999', '2010', '1975', '1972', '2015', '1989', '1994', '1993', '1996', '2013', '1974', '1990', '1995', '2004', '1987', '1973', '1992', '1977', '1985', '2007', '2009', '1986', '1978', '2002', '1979', '2006', '2008', '2005', '1980'] \n",
      " ['1993', '1985', '1971', '1975', '1996', '1979', '1978', '1989', '1986', '1971', '1985', '1975', '1998', '1972', '1990', '2011', '1982', '1976', '1998', '1977', '1998', '1979', '1990', '1976', '1984', '1982', '1989', '1976', '1971', '1986', '1985', '1972', '1987', '1991', '2012', '1983', '1973', '1988', '1986', '1981', '1975', '1987', '1994', '2002', '1982', '1998', '1991', '1994', '1984', '1975', '1975', '1991', '1992', '1971', '1986', '1987', '1995', '1973', '2010', '1982', '1995', '1983', '1992', '1987', '1994', '1978', '1991', '1991', '1987', '1985', '1982', '1989', '1977', '1974', '1981', '1979', '1987', '1999', '1980', '1998', '1981', '1978', '1974', '2000', '1971', '1981', '1993', '1997', '1982', '1989', '1995', '1991', '2004', '1976', '1990', '1974', '1986', '1991', '1972', '1983', '1992', '1997', '2002', '1982', '1974', '1978', '2006', '1983', '1974', '2011', '1995', '1972', '1982', '1992', '1997', '1989', '1987', '1977', '2006', '1992', '1998', '1996', '1990', '1977', '1996', '2001', '2004', '1982', '2002', '1972', '1987', '1996', '2007', '1994', '1981', '1985', '1985', '1983', '1992', '2012', '1995', '2016', '1996', '1992', '2003', '1999', '2004', '2001', '1978', '1989', '1980', '1990', '2015', '1972', '1974', '1974', '1974', '1990', '2000', '2001', '2007', '2016', '1974', '1994', '1978', '1975', '1996', '1979', '1986', '1995', '2011', '1972', '1993', '2006', '1988', '1983', '2010', '1990', '1985', '1982', '1988', '1995', '1974', '2008', '2002', '1985', '2016', '2008', '2004', '1977', '2000', '1972', '1993', '1995', '1990', '2001', '1994', '1981', '2013', '1986', '1978', '1999', '1989', '1995', '1976', '1983', '1990', '2004', '2012', '1983', '1989', '1992', '1999', '1997', '1974', '1981', '1988', '2011', '1989', '1975', '2012', '1980', '1998', '1974', '1998', '1972', '1991', '2011', '1985', '2011', '1986', '2016', '1977', '1990', '2003', '2015', '1995', '1976', '1995', '1978', '2011', '2004', '2010', '2012', '2013', '1990', '1981', '1983', '1995', '1993', '2005', '1998', '2007', '2016', '1979', '2014', '1988', '2015', '1976', '1979', '2000', '1986', '2002', '1981', '2007', '1989', '1999', '1980', '2009', '1992', '2006', '2008', '1993', '2010', '1985', '1984', '1986', '2007', '1974', '2013', '1985', '2004', '2012', '1977', '1987', '1983', '2013', '2010', '2009', '2007', '2011', '2004', '1995', '2008', '1983', '1983', '1979', '2009', '1993', '1974', '1994', '1992', '2004', '1977', '2002', '2000', '2004', '2006', '1994', '1977', '1992', '1995', '1989', '1978', '2007', '1976', '2011', '1975', '1978', '1975', '2012', '1999', '1991', '1973', '1996', '2007', '1995', '1999', '2001', '2000', '1988', '1993', '1974', '1997', '1986', '1973', '1978', '2007', '1998', '2005', '1980', '2007', '1976', '1998', '2005', '1973', '2005', '1980', '2005', '1987', '2004', '1974', '1986', '1997', '1993', '1981', '2003', '1993', '1983', '1994', '2008', '1980', '2003', '1975', '2010', '1997', '2014', '2001', '1986', '1978', '1975', '2009', '1995', '1989', '2000', '1973', '1999', '2001', '1978', '2009', '1973', '2014', '1975', '2012', '1999', '1989', '2009', '1998', '1995', '2009', '1988', '2007', '2000', '2012', '2001', '1977', '2008', '1983', '1979', '1992', '2008', '2014', '1974', '1981', '1986', '1973', '1994', '1999', '2010', '1994', '2007', '2010', '1990', '2016', '2004', '1973', '1987', '2000', '1975', '1977', '2000', '1984', '1973', '1986', '1979', '2004', '1984', '2016', '2007', '1982', '1981', '2013', '1999', '2006', '1978', '1989', '1974', '1986', '1983', '2014', '1989', '1980', '1992', '2000', '1981', '2008', '2002', '1985', '2010', '2002', '1994', '2004', '2003', '1991', '1982', '1984', '2000', '2001', '1982', '1998', '2012', '1991', '1988', '2014', '2016', '1976', '1981', '2011', '1997', '2003', '1983', '1999', '2010', '1975', '1972', '2015', '1989', '1994', '1993', '1996', '2013', '1974', '1990', '1995', '2004', '1987', '1973', '1992', '1977', '1985', '2007', '2009', '1986', '1978', '2002', '1979', '2006', '2008', '2005', '1980']\n",
      "[2, 9, 28, 53, 84, 13, 31, 98, 111, 129, 153, 171, 191, 225, 474, 36, 57, 323, 335, 345, 375, 380, 405, 415, 422, 486, 73, 82, 95, 104, 108, 154, 155, 156, 162, 182, 214, 223, 278, 299, 332, 351, 402, 436, 481, 3, 11, 40, 49, 50, 165, 219, 317, 319, 363, 370, 382, 418, 473, 17, 23, 27, 93, 204, 237, 258, 315, 342, 465, 19, 72, 117, 123, 189, 232, 283, 303, 309, 395, 419, 488, 6, 65, 81, 105, 148, 164, 200, 239, 313, 318, 336, 369, 378, 434, 493, 5, 21, 75, 167, 254, 259, 296, 398, 424, 495, 78, 150, 221, 267, 340, 347, 361, 441, 499, 39, 74, 80, 85, 134, 197, 215, 246, 263, 355, 403, 430, 444, 466, 16, 25, 44, 59, 70, 88, 103, 112, 127, 179, 429, 454, 458, 35, 61, 99, 107, 137, 175, 205, 209, 247, 285, 294, 295, 358, 397, 438, 470, 24, 48, 275, 421, 426, 455, 1, 10, 30, 69, 135, 136, 178, 185, 228, 274, 280, 447, 489, 8, 29, 38, 54, 96, 168, 199, 230, 261, 276, 334, 352, 368, 404, 423, 437, 492, 32, 41, 55, 63, 68, 76, 116, 130, 284, 349, 416, 485, 37, 174, 180, 216, 256, 330, 390, 462, 7, 26, 71, 89, 115, 149, 202, 210, 218, 265, 312, 373, 385, 435, 440, 476, 14, 22, 94, 122, 151, 157, 177, 194, 206, 233, 245, 412, 482, 33, 46, 51, 66, 67, 91, 97, 226, 322, 453, 461, 52, 62, 100, 113, 119, 138, 143, 211, 269, 301, 310, 399, 442, 487, 0, 86, 172, 192, 249, 272, 298, 331, 354, 357, 478, 42, 47, 64, 133, 163, 196, 300, 308, 359, 406, 409, 450, 477, 56, 60, 90, 110, 140, 169, 181, 193, 203, 236, 238, 248, 292, 311, 326, 372, 388, 483, 4, 121, 124, 131, 142, 166, 324, 479, 87, 101, 114, 213, 333, 353, 365, 468, 12, 18, 20, 45, 79, 120, 222, 224, 251, 338, 343, 387, 459, 77, 145, 201, 212, 266, 321, 327, 376, 384, 407, 432, 471, 83, 158, 190, 260, 305, 329, 374, 392, 417, 420, 443, 456, 125, 147, 159, 195, 328, 367, 377, 394, 457, 43, 102, 128, 184, 262, 304, 446, 449, 494, 144, 234, 356, 362, 452, 469, 92, 126, 146, 188, 207, 241, 281, 291, 302, 306, 350, 414, 425, 451, 484, 250, 339, 344, 346, 348, 498, 106, 118, 173, 270, 307, 433, 496, 132, 160, 252, 264, 277, 289, 314, 325, 337, 341, 391, 410, 428, 490, 183, 187, 271, 293, 360, 396, 400, 445, 497, 268, 288, 297, 371, 379, 386, 389, 491, 58, 176, 242, 273, 287, 364, 408, 411, 448, 472, 15, 109, 170, 217, 227, 229, 240, 290, 316, 467, 34, 139, 208, 220, 243, 282, 320, 383, 393, 460, 198, 244, 279, 286, 431, 480, 255, 366, 381, 401, 439, 463, 152, 235, 257, 475, 141, 161, 186, 231, 253, 413, 427, 464]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(day,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,month,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,years)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43myears\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[idx])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "def date_sorter():\n",
    "    \n",
    "    order = None\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    test = df.str.findall(r'(\\d?\\d)[\\/\\-](\\d?\\d)[\\/\\-](\\d{2})(?!\\d)')\n",
    "    reg_month = '((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))'\n",
    "    #test = test + df.str.findall('r\"' + reg_month + '[\\s\\-\\.]+(?:\\d?\\d)[,\\s\\-]+(?:\\d{4}|\\d{2})')\n",
    "    \n",
    "    test = test + df.str.findall(r'((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))[\\s\\-\\.]+((?:\\d?\\d)(?!\\d))[,\\s\\-]+((?:\\d{4}|\\d{2}))')\n",
    "    test = test + df.str.findall(r'((?:\\d?\\d))[,\\s\\-]+((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))[\\s\\-\\.,]+((?:\\d{4}|\\d{2}))')\n",
    "    test = test + df.str.findall(r'((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))[\\s]((?:\\d?\\d\\w*)(?!\\d))[,\\s\\-]*((?:\\d{4}|\\d{2}))')\n",
    "    test = test+ df.str.findall(r'(?<!(\\d{2}\\s))((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?))[\\s]((?:\\d{4}))')\n",
    "    test = test + df.str.findall(r'((?:\\d{1,2}))?[\\/]*((?:\\d{4}))')\n",
    "    test = test + df.str.findall(r'((?:Jan(?:[ua]+ry)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:[em]+ber)?))[\\s\\,\\-]((?:\\d?\\d\\w*)(?!\\d))?[,\\s\\-]*((?:\\d{4}|\\d{2}))')\n",
    "    \n",
    "    #df_test = test.apply(pd.Series)\n",
    "    #df_test.columns = []\n",
    "\n",
    "    #raise NotImplementedError()\n",
    "    return test # Your answer here\n",
    "\n",
    "test = date_sorter()\n",
    "years = [item[0][-1] for item in test]\n",
    "day = [item[0][0] if len(item[0][0])!=0 else 1 for item in test]\n",
    "month = [item[0][1] for item in test]\n",
    "\n",
    "years = ['19'+year if len(year)<4 else year for year in years ]\n",
    "idx = sorted(range(len(years)), key=lambda k: years[k])\n",
    "\n",
    "mask_test = test.str.len()==0\n",
    "#print(test[mask_test].index)\n",
    "print(day,'\\n',month,'\\n',years)\n",
    "print(idx)\n",
    "print(years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('12', '01', '73')]\n",
      "(5/11/85) Crt-1.96, BUN-26; AST/ALT-16/22; WBC_12.6Activities of Daily Living (ADL) Bathing: Independent\n",
      "\n",
      "    Month  Day  Year\n",
      "10     26   16  1922\n",
      "9       4   10  1971\n",
      "84      5   18  1971\n",
      "53      7   11  1971\n",
      "2       7    8  1971\n",
      "..    ...  ...   ...\n",
      "464  2016    8  2016\n",
      "253  2016  Feb  2016\n",
      "141    30  May  2016\n",
      "231    50  May  2016\n",
      "427     6    5  2016\n",
      "\n",
      "[500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class Token(NamedTuple):\n",
    "    type: str\n",
    "    value: str\n",
    "    line: int\n",
    "    column: int\n",
    "\n",
    "def tokenize(code,i):\n",
    "    \n",
    "    token_specification = [\n",
    "        ('month_day_num', r'((?<![\\/\\-]\\d{1}[\\/\\-])(?<![\\/\\-]\\d{2}[\\/\\-])(?:\\d?\\d)(?!\\d{2}))'),  # Integer or decimal number\n",
    "        ('month_alfa',   r'(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)'),           # Assignment operator\n",
    "        ('year',      r'(?<!\\d)(?:\\d{4}|\\d{2})\\b')\n",
    "    ]\n",
    "    tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_specification)\n",
    "    \n",
    "    temp_group = []\n",
    "    year_group = []\n",
    "    date_group = []\n",
    "    line_num = i\n",
    "    line_start = 0\n",
    "\n",
    "    k = 0\n",
    "    for mo in re.finditer(tok_regex, code):\n",
    "        kind = mo.lastgroup\n",
    "        value = mo.group()\n",
    "        column = mo.start() - line_start\n",
    "        \n",
    "        \n",
    "        #temp_group.append(mo.group())\n",
    "        \n",
    "        \n",
    "        if mo.group('year')!= None:\n",
    "            year_group.append((line_num,mo.group('year'),k))\n",
    "        \n",
    "        temp_group.append(mo.group())\n",
    "        \n",
    "        k +=1    \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    if   len(temp_group) == 1:\n",
    "        temp_group=[1,1,1]\n",
    "        year_group.append((line_num,mo.group(),k))\n",
    "    \n",
    "    l = year_group[-1][-1]     \n",
    "    #print(year_group)\n",
    "\n",
    "    date_group.append((temp_group[l-2],temp_group[l-1],year_group[-1][-2])) \n",
    "    \n",
    "    \n",
    "\n",
    "        #yield Token(kind, value, line_num, column)\n",
    "    \n",
    "    return temp_group, year_group,date_group\n",
    "        \n",
    "   \n",
    "\n",
    "statements = df\n",
    "#statements = ['2004','Mar 20th, 1992','3-4-52']       \n",
    "#a = pd.DataFrame(index=[0],columns=['day','month','year'])\n",
    "\n",
    "i=0\n",
    "temp =[]\n",
    "temp_year = []\n",
    "temp_date = []\n",
    "for s in statements:\n",
    "    temp.append(tokenize(s.strip(),i)[0])\n",
    "    temp_year.append(tokenize(s.strip(),i)[1])\n",
    "    temp_date.append(tokenize(s.strip(),i)[2])\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "#idx = [x if len(temp[x])>3 else [] for x in range(0,500)]\n",
    "\n",
    "print(temp_date[57])\n",
    "print(df[10])\n",
    "\n",
    "final_df = pd.DataFrame([t for lst in temp_date for t in lst],columns=['Month','Day','Year'])\n",
    "final_df['Year'] = ['19'+year if len(year)<4 else year for year in final_df['Year'] ]\n",
    "final_df.sort_values(['Year','Month','Day'],inplace=True,kind='mergesort',ascending=[True,True,True])\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0843c1f0ad2aaa45fa9ac4012f1aa43",
     "grade": true,
     "grade_id": "cell-373f878879c00996",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e2f5bb6bab79c07a81ec366c46c4d49",
     "grade": true,
     "grade_id": "cell-0ebae76e6cd794be",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "LvcWI",
   "launcher_item_id": "krne9",
   "part_id": "Mkp1I"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
